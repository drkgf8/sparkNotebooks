
---
title: "7-SparkR"
output:
  html_document:
    toc: true
---

# SparkR Overview

SparkR is an R package that provides a light-weight frontend to use Apache Spark from R. 

The simplest way to create a DataFrame is to convert a local R data frame into a SparkR DataFrame. The following cell creates a DataFrame using the faithful dataset from R.

```{r}
library(SparkR)
df <- createDataFrame(sqlContext, faithful)
```

Display the `df` dataframe using the `head` method.

```{r}
head(df)
```

# Read Data Sources using Spark SQL

You can create DataFrames from stored data such as CSV and JSON using `read.df`.  

This method takes in the SQLContext, the path for the file to load and the type of data source. SparkR supports reading JSON and Parquet files natively and through Spark Packages you can find data source connectors for popular file formats like CSV and Avro.

You can also use `display` to format the loaded data frame.

```{r}
markets <- read.df("dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/data-001/market_data.csv", "com.databricks.spark.csv", header="true", inferSchema="true")
display(markets)
```

`read.df` attempted to infer the schmea for the CSV data.

use `printSchema` to show the schema for `markets`

```{r}
printSchema(markets)
```

You can also load data from Spark SQL tables.  First load some data into a temp table.

```{r}
taxes2013 <- read.df("dbfs:/databricks-datasets/data.gov/irs_zip_code_data/data-001/2013_soi_zipcode_agi.csv","com.databricks.spark.csv", header="true", inferSchema="true")
createOrReplaceTempView(taxes2013, "taxes2013")
```

Next cleanup the data and store in a persistent table.

```{r}
%sql
DROP TABLE IF EXISTS cleaned_taxes;

CREATE TABLE cleaned_taxes AS
SELECT 
  state, 
  int(zipcode / 10) as zipcode,
  int(mars1) as single_returns,
  int(mars2) as joint_returns,
  int(numdep) as numdep,
  double(A02650) as total_income_amount,
  double(A00300) as taxable_interest_amount,
  double(a01000) as net_capital_gains,
  double(a00900) as biz_net_income
FROM taxes2013
```

Finally, load the data into a dataframe using the persistent table as a source.

```{r}
taxes <- sql("SELECT * FROM cleaned_taxes")
display(taxes)
```

Let's revist the `faithful` dataset.

`select` the `eruptions` column from `df`.  Store the result in `eruptions`.

Use `head` to show `eruptions`.

```{r}
eruptions <- select(df, df$eruptions)

head(eruptions)
```

You can also specify the column name using a string.

```{r}
eruptions2 <- select(df, "eruptions")
head(eruptions)
```

You can `filter` a dataset too.

Use `filter` to show only the rows where `waiting` is less than 50.  Store the results in `waiting`.

Show `waiting`.

```{r}
waiting <- filter(df, df$waiting < 50)
display(waiting)
```

Next group `df` by "waiting" and count the occurrences of each waiting period.

Store the results in `waiting_counts` and show `waiting_counts`.

```{r}
waiting_counts <- count(groupBy(df, df$waiting))
display(waiting_counts)
```

Display the most common waiting times using `head`.

First `arrange` the `waiting_times` dataframe in descending order by count.  Store the results in `sorted_waiting_times`, then use `head` to show the most common.

```{r}
sorted_waiting_times <- arrange(waiting_counts, desc(waiting_counts$count))
head(sorted_waiting_times)
```

You can apply a transformation to a column, then assign the result back to a new column on the same dataframe.

Tranform `waiting` to `waiting_seconds` and store the result as a new column on `sorted_waiting_times`.  Show `sorted_waiting_times`.

```{r}
sorted_waiting_times$waiting_seconds <- sorted_waiting_times$waiting * 60
display(sorted_waiting_times)
```

Finally we can access Spark MLLib easily from R.  Let's load the `iris` dataset and build a linear regression model.

```{r}
iris_data <- createDataFrame(iris)

model <- glm(Sepal_Length ~ Sepal_Width + Species, data = iris_data, family = "gaussian")

summary(model)
```

