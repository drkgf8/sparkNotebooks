{"cells":[{"cell_type":"markdown","source":["In our notebook we have access to sqlContext"],"metadata":{}},{"cell_type":"code","source":["sqlContext"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.context.HiveContext at 0x7fc139c061d0&gt;\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["Spark Context"],"metadata":{}},{"cell_type":"code","source":["sc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">2</span><span class=\"ansired\">]: </span>&lt;SparkContext master=local[8] appName=Databricks Shell&gt;\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["And SparkSession"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.session.SparkSession at 0x7fc139c06290&gt;\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["We can use the Spark Context to create a small python range that will provide a return type of `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["firstDataFrame = spark.range(1000000)\nprint firstDataFrame"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DataFrame[id: bigint]\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["You might have expected the last command to print the values of `firstDataFrame`.  But the `range` command is a **transformation** operation, and Spark will wait until the values are needed before it calculates them.  Spark will know a value is needed when you execute an **action** operation.  \n\n`show()` is an **action**:"],"metadata":{}},{"cell_type":"code","source":["firstDataFrame.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n id|\n+---+\n  0|\n  1|\n  2|\n  3|\n  4|\n  5|\n  6|\n  7|\n  8|\n  9|\n 10|\n 11|\n 12|\n 13|\n 14|\n 15|\n 16|\n 17|\n 18|\n 19|\n+---+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["To show you the values in the DataFrame, Spark must know the values first.  When you call `show()` you force Spark to execute all the transformations needed in order to know the values in the DataFrame, and Spark will automatically do this at the right time.\n\nHere are some simple examples of transformations and actions.\n\nTransformations (lazy) - select, distinct, groupBy, sum, orderBy, filter, limit\nActions - show, count, collect, save"],"metadata":{}},{"cell_type":"markdown","source":["Try another transformation.  Tell Spark that we want to create a `secondDataFrame` by multiplying 2 by each value in the `firstDataFrame`."],"metadata":{}},{"cell_type":"code","source":["# select the ID column values and multiply them by 2\nsecondDataFrame = firstDataFrame.selectExpr(\"(id * 2) as value\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["`take` is another action.  This action selects a certain number of elements from the beginning of a dataframe.  Try to take five values from each DataFrame."],"metadata":{}},{"cell_type":"code","source":["# take the first 5 values that we have in our firstDataFrame, use print to see the results\nprint firstDataFrame.take(5)\n# take the first 5 values that we have in our secondDataFrame, use print to see the results\nprint secondDataFrame.take(15)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n[Row(value=0), Row(value=2), Row(value=4), Row(value=6), Row(value=8), Row(value=10), Row(value=12), Row(value=14), Row(value=16), Row(value=18), Row(value=20), Row(value=22), Row(value=24), Row(value=26), Row(value=28)]\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["Spark separates operations into **transformations** (which are evaluated only when needed) and **actions** (which are evaluated immediately) to support an optimization called pipelining.  This lets you decide what you want the data to look like, while leaving it up to Spark to find the most efficient way to calculate your results.\n\n![transformations and actions](http://training.databricks.com/databricks_guide/gentle_introduction/pipeline.png)"],"metadata":{}}],"metadata":{"name":"4-TransformationActions","notebookId":2175376238036011},"nbformat":4,"nbformat_minor":0}
